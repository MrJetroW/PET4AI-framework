# BlueDragon Podcast â€” S02E01  
**Guest:** Dr. Krishna Chaitanya Rao Kathala  
**Date:** December 23, 2024  
**Topic:** Privacy-Enhancing Technologies in the Age of AI Innovation  
**Watch the episode:** [YouTube Link](https://youtu.be/FvonnGxgh-Y?si=xzjf5pL3E16tkXL-)  

---

## Transcript

Well, thank you everybody for joining. This is the first episode of season two of the BlueDragon podcast. And today we're going to start with a wonderful guest because I have today the author of the book 'Privacy in the Age of Innovation, AI Solutions for Information Security'. It is Krishna. Krishna, thank you very much for joining. Welcome. Thank you. Thank you so much, Jetro, for having me on the podcast. And for those of us who don't know you yet, can you take a minute and describe your background and how you ended up writing this book on privacy and AI in the age of innovation? Sure. First of all, thank you so much for having me. And I'm delighted to be here. And my professional journey spans over 10 plus years, focusing on AI, data science and privacy, particularly within the education space. And I have worked extensively on data governance, analytics, and integrating ethical AI practices into institutional framework. And I have my PhD in education, my master's in data science. And that's so interesting. And the inspiration for this book came from my hands-on experience in managing sensitive data environments, like universities where compliance with loss, like FERPA in the US and GDPR for international students is kind of non-negotiable. So I collaborated this book with my co-author, Ranadeep. He's a senior engineer at Zoom company. So we both kind of pulled it together. And we also presented some of the pressing need around privacy and PETs, of course, and some of the best practices and use cases. Great. Wonderful. And so, yeah, on LinkedIn, you have about 7,500 followers and you're based in the United States, if I'm not mistaken. Yes, that's correct. Yeah, I am currently based in Boston, Massachusetts. Wonderful. And so when I saw your book, and I think it just came out recently, like a few months ago, I bought the book. I read it. Like within a week, I had it completely finished. I really love the concepts that you're explaining, but I'm sure that many of our audience is new to the concept of a PET, a privacy enhancing technology. So what is it and why is it important? Sure. So, yeah, I think when it comes to the PET, again, like for the folks, especially who are in the privacy space particularly, and it's a privacy enhanced technology, and it is essentially crucial because there is in the space of information, again, we are living in that digital space where the privacy is very on the homomorphic encryption, and there is a lot of that synthetic data that is gaining a lot of traction. And I think it is very crucial to enable secure AI applications without really exposing the raw data. And that is one of the piece where we were really focusing on in this book. Yeah. So if you say a PET, like a privacy enhancing technology, you already mentioned some concepts like homomorphic encryption and synthetic data, but let's take one step back. Like, why would an organization that is adopting AI, like many are, why would they need a PET? What is the problem that you could solve with a PET? Yeah, that's a very good question. So as we see within the organizations or within the institution space, there is increasing need for privacy enhancing solutions. And I think to address this need, I think I would love to start with the regulatory compliance aspect. And we see specifically like GDPR in Europe or there's California Consumer Privacy Act in US. I think these laws really require organizations to handle personal data with care. And there's a lot of the transparency that comes into play and there's consent, there's minimal data exposure. So there are two aspects to the PET, which is again, there is differential privacy, and then there's a federated learning component. So when it comes to the differential privacy aspect, which is specifically about adding the statistical noise to the data sets, which ensures that individual records remain anonymous while still allowing the aggregated insight. So especially when it comes to my work within the higher education space, as I heavily work on the student's data, and it's often that there's a lot of personal information like their race, their ethnicity, their gender, and their date of birth, their SSN records, and things like that. So you make sure that not that individual information is being identified, but making sure that it's in place. And then there is a federated learning component, which really allows AI model to train across decentralized data without moving the sensitive information and of course, ensuring that the cross-border data regulations aspect in place. All right, so if you take a look at a regular organization, up until a year or two years ago, they were storing data from all different data points, whether it was a CRM or an ERP or just an M365 of Microsoft. And there were already some privacy controls in place because we have GDPR since 2018. But now since let's say a year or two with everything at chat GPT and all of the AIs or more mainstream, there is a new need, a new problem that is arising. Can you explain a bit more about what the rise of AI and the rise of privacy, how those correlate together? Sure. Yeah, I think when it comes to the AI aspect, I think one of the significant trend that we see in the space of AI, which again, kind of going back in terms of how do we integrate AI into zero trust security models and how do we make sure to manage the cybersecurity threats? And in fact, US is kind of really investing a lot in the privacy space. And recently the Federal Act, which is specifically centered around AI as again, the AI is growing and there is lack of awareness aspect. And now with the combination of AI and machine learning as AI systems thrive on data, but again, their reliance on the large data sets significantly raises privacy concerns. And again, the new need kind of going back to the PTs that I was talking about, which is centered around federated learning, there is synthetic data, and then there is also securing multi-party computation really to make possible to train AI models while preserving the privacy and the fairness aspect. And one of the examples for the audience from the non-technical background. So if you really look at the multinational retail company and the way that they approach to the federated learning is to train its recommendation engine across multiple regions without really transferring or without really exposing the consumer data. So yeah, I think there are the growing interest and there's a lot of investments happening. But again, it's always a challenging aspect of how do we make sure that AI is, the data that we are using is privacy compliance and ensuring that the regulatory aspects are in place. Exactly. So you are an organization and you're moving towards an AI solution. So you unleash it on your data, your organizational data, but not all data is equal. Some are more sensitive than others. And so you want a way to control what the AI can pick up and learn and create a model out of, right? So then PETs come into the picture. Can you share a bit like with the audience, like what are the three or four main techniques of a PET because it's a family, it's an umbrella term. What are the three, four technologies underneath the PET? Sure. So yeah, I think this, the privacy enhanced space is again, kind of growing. I think one of the big piece within the PET is the differential privacy aspect. So I'll try to put it in a very layman terms. So when it comes to the differential privacy, it really ensures that the statistical analysis or queries on the dataset that we are trying to apply, which really doesn't reveal any information about the individual entries. Like let's say, think of you going to a Costco and trying to purchase a wine and you're also purchasing some eggs and a bottle of milk. And you're really looking at not the items, but not as an individual person, but as a group of the people who are purchasing the same set of objects. So without really looking at from the object side of it. And again, the other applications within this is of course, there's healthcare, where they do a lot of patients data across different hospitals without really exposing individual records. And in fact, within the US government space, so as US Census Bureau uses the differential privacy to protect the individual data while publishing, you know, be it demographic statistics and things like that. So again, there are also several benefits. And then the other aspect, the second one would be the federated learning component in the federated learning component, where it really allows AI models to be trained on multiple decentralized devices or even, you know, beat the data sources without really transferring the raw data or your actual source of data. And again, thinking of its application. So again, going back to the healthcare, so, you know, there's collaborative research on AI models for this is prediction across hospitals in different countries, not just from the US, but let's say you're collaborating from Europe, India and US, and how do we ensure that the patient data remain local? And how do you make sure that the data in place? And then the other aspect, which is the homomorphic and encryption piece, which again, it, the fundamental idea of this component is it really allows the encryption to be performed on the encrypted data without really decrypting it. And again, this component will really ensure that the data remains protected, even during the data processing times, as, you know, companies always try to, you know, invest in a lot of efforts in the data, data preparation, data cleaning, data processing space. So yeah, I think these are the three interesting aspects that I wanted to share. Great. So it is about differential privacy. It was about the animals of the homomorphic encryption. And what was the middle one again, the second? Federated learning. Correct. So these three are more apart, but are all part of the PET. Now, the question that I have is this, is this privacy enhancing technologies? Are they used during the training phase of an AI model or when it's being consumed, like in the inference phase? Sure. So, so yeah, I think kind of going back to, you know, its approaches, it's basically, you know, the PETs can be used in the various phases in the AI, if we think of like as an AI lifecycle. So starting with the data collection phase, right? So again, it PETs can really ensure that the data collected for the AI development kind of complies within the privacy regulations and also it maintains the user trust. And if we think of the differential privacy space, right, so it kind of adds noise to the data at the collection stage, which again, ensuring that the individual data points cannot be identified. And then there is other concept, it's called zero knowledge proofs. What it really tells is it really allows to verify of the user provided data, again, you know, beat age, beat, you know, location, again, without exposing any sensitive information. And then if we think of the data preparation phase, you know, again, before we even, you know, start thinking about building like an AI model where, you know, the data must be often clean, it should be integrated and it should be shared across different teams within the organization. So again, starting with the synthetic data generation, SAS, you know, it creates, you know, various artificial data sets that kind of retain the statistical power of the real data without really exposing to the sensitive details. And then coming to the core space, which is the model training space and the way that I see the PET is coming to space in the model training space, again, kind of going back to the two core principles, which is again, the federated learning component where, you know, it trains the AI models across decentralized devices or even the data sources without transferring the raw data. And then the other component, which is the homomorphic encryption, what really does during the model preparation stage is it enables the computations on encrypted data, ensuring that the raw data that we pull from the system remains private and then thinking of moving to the deployment phase, right again. Now this is where the challenging part comes in because you need to ensure that, you know, data that you are publishing is privacy compliant, be it, you know, the GDPR, FERPA. And again, each country has its own regulation. So again, then the differential privacy component would come in to ensure that outputs are even beat, you know, any predictions do not really adverse any, you know, statistical or any sensitive training data that we've trained. And then there's the homomorphic encryption again, which really does is to ensure that it protects the data in the real time and especially when we are doing the predictions or recommendations. And then the fifth stage, which in the GAN, if you're thinking, if you are thinking that AI lifecycle, which is the model inference phase, here the way that I see is as AI models process real time data to generate predictions or even decisions. So one of the core principles within that, the PET, is to ensure that the user data remains private during these interactions, like, you know, one of the core principles that I touched, which is the secure multi-party computation. GAN, what it really does is it processes the user query or inputs without really exposing the underlying data of the model to the provider or, you know, the service provider that, you know, which you are really using, be it the other cloud party or, you know, be it Amazon or different use case. And then, of course, then the post deployment phase, there is this monitoring aspect that would come into play where, again, the differential privacy comes in and then the zero-knowledge proofs where, you know, you verify system updates and interactions without really exposing the sensitive models. So, yeah, I kind of see the PETs could come in, you know, all the phases of the development lifecycle. Really nice. That is very practical, actually, because when I talk to organizations and I'm based in Belgium, they are when I talk to CISOs, Chief Information Security Officers or Data Protection Officers, they feel that the business forces them to allow AI. The co-pilot gets activated, chat GPT gets activated. When they're developing their own software in the house, the development team says, we want to use, I'm just saying, an Azure OpenAI, you know, because it's a very, and then the CISOs are like, how do I deal with the security and with the privacy of all of this? And what you're saying is that throughout the entire lifecycle, PETs are a solution, a technological solution to secure and keep things private when needed. I really like that. So just for my recollection, the phases were like data preparation, then you go to model training, right? And at the end, you have model inference, which is the consumption and using the model. But then what was the step or phase three and four again? So the training, then the AI model deployment phase, and then the model inference phase that comes after that. And then often companies don't really look at the post deployment, which is the monitoring phase. So that is where the key component that I was talking about, the federated learning and the synthetic zero knowledge proofs. Really nice, because that gives like a CISOs like a model that they can do from beginning till the end. Wonderful, great. So for the audience for IT leaders and security officers, PETs from beginning till the end have a role to play. But then occasionally comes the next question, which is, Ama CISO, imagine, how do I choose which PET to use when? What advice can you give these leaders? Sure. Actually, that's a tough question to start with. Because again, like, you know, every company has, you know, different focus, different vision. So I think the fundamental piece that I would love to start is I think identifying the goals and requirements, right? Like what you really want to achieve. So maybe, you know, when I pick up any project, I always start with asking the fundamental questions by myself. So like, are you ensuring the compliance with regulations like GDPR, you know, CCPA, and do you need to, you know, even enable collaborations across multiple parties or multiple regions? Because again, companies may have their offices in different places across the world. So how do you ensure that, you know, and then are you prioritizing user trust and transparency? That is something that I always kind of, you know, strengthen. And again, when it comes to the PET aspects of a regulatory compliance, I always consider the differential privacy and then the homomorphic encryption. And then when it comes to the collaborative aspect, I always use the federated learning and then the secure multi party computation. And then when there is, you know, the real time privacy inference, so I heavily use the homomorphic encryption again, or the secure multi party computation. And then, you know, once you assess, once you have that understanding, then I'm kind of start thinking about evaluating the data sensitivity and use it. So like data quality or data quality, quantity, and then like, how do you kind of bucket the data into like, you know, highly sensitive, then, you know, there's moderately sensitive data, and then there is low sensitive data. So again, for a highly sensitive, I think homomorphic encryption is something that's popular across the board, which is again, really to process the encrypted data securely, or even federated learning to really avoid that transferring the raw data. And then when there's a moderate sensitive data in place, then you would look at the differential privacy aspect. And then when you have the low sensitivity or, you know, that data, you can consider using synthetic data generation aspect. And then again, kind of going back to the lifecycle, right, like, if I kind of start thinking of the data maturity landscape, right, so every organization is in the different stages in the data maturity landscape. So really, as I was saying, in those six phases of that AI lifecycle that I was talking about, so really assessing what stage we are at, and how do we best invest in the PETs into the companies, because again, companies don't really realize the importance of PETs, but again, within the model development, model deployment, and also, PETs can come into play, you know, when you're monitoring the post deployment initiatives. And when you're even thinking of putting together an impact report straight out of your project that you've done, so you really need to make sure that some of the quantified aspects from your PET, you know, model development, model deployment, that really builds the user trust from the consumer standpoint. And, you know, again, considering the computational complexity, right, so when there's a low overhead, again, the differential privacy and synthetic data generation are kind of less resource intensive, because again, as a leader, we also need to keep in mind that, you know, we are keeping under budget again with the cloud costs, how do you ensure, and then there's a high overload of data, so then the homomorphic encryption, and then the SMPC, which again is secure multi-party computation aspect, and then there is also the accountability aspect that I always kind of questions myself in terms of, again, when I pick these different models in, you know, from the concept to launch, federated learning, again, which to choose when there is a lot of that collaboration involved, then the federated learning, and then SMPC for secure computations, again, when you're involving multiple stakeholders. And then lastly, again, I go back to my first question in terms of, am I aligning, you know, my data to the regulatory and the ethical requirement aspects? So one of the pieces at which currently I'm working on the explainable AI aspect in terms of, you know, not just PET, of course, PET complements, but again, how do you ensure, you know, there's fairness, there's the transparency aspect, and, you know, how do we ensure that we are adhering to the data localization loss? Because again, you know, these laws keep updating every time by now and then, so it's kind of hard to keep up on things, but again, you need to make sure that it's in compliance. Really nice. That is a very good explanation. So thank you for that. And yeah, we're living here in Europe. So we have the strictest legislation of the world when it comes to privacy and data security. So let's say we're the tip of the spear. But what I also see a lot of organizations struggle with is they're activating and enabling all these AI tools. But I see a couple of things. I see organizations that are just using it as a SaaS solution. So software as a service. Think of like chat GPT and a Copilot. And then you are kind of dependent on an open AI or a Microsoft for the options to secure the data because it's just SaaS. And then typically it is like data labeling. You have to label your documents and emails and databases so that it knows what it can use. But if you go to the other type of organizations, those that are building their own software and thus are using more of a platform as a service like Azure open AI or they're using the open AI APIs, non-Microsoft ones let's say or other ones. Then they have more control over PETs and how they activate them. What kind of advice can you give to those companies that are building their own software? They just activated some wonderful open AI APIs. And now comes the CISO that says, hey guys, we need to keep this secure. What would you recommend to do? Sure. So yeah, I think with the advent of the large language models and especially within the chat GPT or the Microsoft co-pilot piece, again, with the use of each tool and it has its own pros and cons. So I think it again kind of goes back to understanding the needs of your target market. And of course, like with if you're leveraging the chat GPT API, again, your data might be used by the chat GPT to train its own internal models. But let's say if you are a Microsoft secure organization where you're leveraging Copilot APIs. So one of the advantage is all the data that you're using for your training purposes within your organization will remain within your organization with the Microsoft Copilot privacy aspect. So personally, our institution and our organization is a Microsoft Copilot organization. So we don't really train our data on any chat GPT APIs. So yeah, I think it kind of goes back to what type of the organization and what kind of the tools that you're using for your organization. So I think that kind of comes to the question. Yeah, exactly. Now what I also hear more and more is the concept of confidential computing, right? But then that basically is that basically the same as a homomorphic encryption or is that some something different? Sure. So yeah, I think again, kind of going back to both the homomorphic encryption and then the confidential computing aspect. So again, it really centers around the data privacy aspect. So within the homomorphic encryption, again, it's a type of a form where the encryption allows computations to be performed directly on the encrypted data without really decrypting it. Again, I'm trying to recollect the three components within the homomorphic encryption, which is I think the partial homomorphic encryption where it kind of supports a single type of operation, be it addition or multiplication. And then there is the other aspect called somewhat homomorphic encryption, which again, really allows the limited number of operations and making it suitable for the complex computations. And then when it comes to its applications or when I'm looking at the computational computing aspect. So again, in that space, I think it really looks at the algorithms and the mathematical models and be distributed computing where there's high performance computing and with the edge computing aspect. So again, it's really focusing. That is that synergy between both. So depends on the use case and depends on the company's applications. Okay, great for explaining that. So now let's take our example again. So we have a software or company that is deploying their own software. They're using some AI components. And I'm not talking about the SaaS like chat GPT. But really like we're deploying machine learning. But typically machine learning happens in the cloud because that's where the most compute power is. What recommendations can you give CISOs and DPOs on securing the data that is used for machine learning in the cloud? Got it. That's a pretty good question. And often, you know, the company leaders are really interested in like, how do I ensure, you know, my data is safe and secure when I'm like kind of pushing the data to the cloud? I think one of the piece that one of the first piece that I would kind of think is how do I adopt the privacy by design principles? Again, what I meant by the design principles is like, you know, incorporating the security and privacy features into the design and implementation of your AI and ML systems within the cloud infrastructure. So one of the, you know, action items that I would take within that is, you know, implementing encrypting by encryption by default, where all the data at the rest and in transit and using the PDS, again, going back to the federated learning or, you know, differential privacy to ensure that the data is privacy and, you know, especially using your AI model building. And then the second aspect that I would look at is the data encryption at all stages. Like I talked about the different phases in the AI life cycle. So encryption at the rest where, you know, how do you ensure that the data is secure? And it is meeting that encryption standards, which is AES 256 to secure the data stored in the cloud. And then there is ensure the encryption is in transit where the secure data transfers using the transport layer security or beat VPNs that if you're using within the system. And then the other aspect that the best practice is the access control management system. So because again, with the different people using different systems, how do you make sure that there's access control management in place? And how do you make sure the MFA, which is the multi-factor authentication in place so that, you know, you're giving access for all cloud accounts. And then you can also assign a granular and role based access within so that, you know, you're not just sharing the data to everyone. But again, you are really limiting the access to only the folks that who are really, who really need and who are going to make decisions based on the data. And then kind of thinking of securing a models in the cloud, again, kind of going back to the model encryption phase where, you know, where you the encrypted on the train data models during the storage and the transmission process. And then one of the piece that which again I've learned from my colleagues is the adverse and robustness because again, when you use these techniques to harden models against the, you know, any adverse attacks or, you know, any phishing attacks or, you know, that kind of exploit the vulnerabilities within the AI algorithms. And then, of course, using the secure APIs again, if you're pulling, you know, data from Shopify API or if you're using different API. So you need to make sure that it is in place. And then the other aspect within the cloud native security tools that which you could leverage again. Let's say if you're using AWS or Google Cloud or even Azure, right, they have their built in tools to enhance the security aspect. So within AWS, this is AWS shield or, you know, you can use Azure TDOS protection to really defend or against any, you know, denial of service attacks and, you know, make sure your cloud audit logs in place. And you can also employ the virtual private cloud for the network isolation. And then I think using the AI specific tools, like if you're using Google's, maybe consider AutoML or like Vertex AI, it has a interesting security features for training and building and the modeling aspect. So yeah, I think, I kind of keep going on these aspects because again, there are different pieces that would come in and it's kind of critical. Even when you're thinking of using any third party tools, so how do you make sure there's a sandbox account in place and you're not really testing on the real data or in the deployed data, but making sure that it's tested on the sandbox account before really pushing to the cloud. Yeah, clear. The reason I'm asking is because in my daily life, I do security assessments, I do cloud security assessments or NIS 2 assessments from Europe, which is the latest, let's say cybersecurity act in Europe. And what I always see is that there is a lot of focus on, let's say the common controls like identity and access management and encryption and data protection and all these things, of course, and training and user awareness. But for me, the whole AI world is new. It is not yet integrated. There's no baseline yet, as far as I know, on how to check that AI is deployed securely. There are some best practices left and right, but no real model or a checklist that helps auditors or assessors like me. And then I read your book and I said, "Hey, these PETs are like very good controls that we can actually verify and give a guidance on." So that's why I really like your book. I think it's one of the first that in an accessible way explains the PETs. And so one of the questions I then ask is, let's say our organization has followed the AI lifecycle and has for each phase, picked a PET and has deployed it. How then can we make sure that it's still effective after six months? Like what are some metrics or indicators to measure the effectiveness of a PET in live production environments? Got it. I think that's a very good question in terms of, if we look at how do you make sure that the PET is sustainable within, even after six months or even after one year. So the first and foremost thing is the documentation. So oftentimes, as we go through that seven stages or six stages that I just talked about, so starting with the data collection phase. So especially if you are looking at the PET within the differential privacy aspect. So one of the piece is, how do you make sure that you're anonymizing the sensitive data and how do you make sure that data during the collection is in compliant with the GDPR or CCPA while maintaining the data utility or even for the research studies as the researchers, if they're trying to undertake any studies, they need to be in compliant with the IRB, it's called Institutional Review Board. And again, kind of going back to it, measuring its success or evaluating. So I think one thing that you could look at for the success evaluation is it's effective on anonymization of sensitive data, which again, reducing the risk of exposing individual identities. And then there is also this enabled compliance within the regional privacy laws, again, across multiple jurisdictions. And then there is also improved user trust, again, which by transparently communicating privacy laws. But again, there are also challenges, right? Like there's all trade-offs between the privacy and then the data utility. What I meant by that is, especially for our data sets that require like high granularity, so you really have to make that trade-off. And then there is also the, now thinking of the data preparation phase in that cycle, again, looking at its success metrics is, how do you accelerate the data preparation workflows without really requiring access to the raw or sensitive data, in terms of looking at its processing time? How do you, are you automating things? Or like, how do you, or like let's say if you're allowing within the internal teams to safely test AI models in the in a development environments or maybe simplify data sharing for a cross departmental collaboration. So that's where the data governance part comes in. I know I haven't talked about the data governance, but again, how do you make sure that, your data is standardized, there is a standard definitions, and how do you make sure that the data is being shared to the stakeholders, especially when you're collaborating with multiple stakeholders across different business units. So yeah, I think the data governance also comes into place in that the data collection and the data preparation phase. And then for the model training, looking at the success metrics is of course, starting with the data localization and the compliance within the broader data regulation aspect. And then one of the challenges that we face within our organization is, it kind of, it could slow the training times, right? Be due to the communication overhead between the federated models or, there are also variations in the data quality and there's data quantity across different centralized nodes. And again, it kind of affects. So one of the improvement and it's one of the success metric that we used is to optimize our communication protocols to reduce the latency and also to improve the model conversion rates. So that could be other metric that you could kind of, include in the success metrics as you're trying to look at like post six months of, and then the monitoring aspect that would come in as, in that specific monitoring phase where you could look at how facilitated secure sharing models perform metrics or sharing between the stakeholders and are you really allowing the transparent data without revealing it's sensitive data that is used in the monitoring phase. And also how do you reduce the insider threats by, ensuring there is no single party had access to all the data, right? I think, oftentimes organizations tend to give or access the data just to one person. And then what if the person, kind of moves or left the organization. So it always happens in Silicon Valley. If you look at some of the interesting stories, so people switch from OpenAI to Microsoft, now Meta comes in with a lucrative package, again, they move to Meta. So again, it's that the corporate gain that comes in. So, but again, if we think from a best practice standpoint, so it is not always a good practice to have all the data access to just one person. So make sure that there's effectiveness, there's cross-stakeholder collaborations, there's multiple people owning different parts of the data so that you're not really losing any of its essence. And I think to all of these aspects, I think the documentation comes to core aspects. So I think, thankfully to my PhD, I think I was not at all a documentation person, but then I think my whole PhD experience kind of really forced me to think and from the documenting standpoint. So now I think it has become a habit as I kind of, and building the PhDs into my workspace. Nice, nice. I also like the fact that you mentioned that the people and the processes are two different dimensions. Like when I say in cybersecurity, or I go to a client and I say, look, cybersecurity, when information security has three dimensions, it's people, it is the policies and the processes, and then it's technology. And most organizations just dive into the tools and the technologies, but they forget people and processes and policies, right? Which is also part of documentation. So I really like the fact that you're mentioning that's also part of securing your AI lifecycle, training your people and having your documentation with the policies and procedures, they know clearly documented and shared. And PETs, like the T stands for technology, is just one dimension, you need the two others as well. So I like that addition there. Now, another question is, when you move to cloud and many organizations in Western Europe are, and I'm sure the US is already ahead of us on that point, they are then coming into what we call the shared responsibility between the cloud provider and the organization using the cloud. And one of the biggest misconceptions I see in that my clients have is like, Jetro, I'm moving in the cloud, so Microsoft is responsible for my data security, right? And it's like, no, there is a shared responsibility, you also have always a responsibility. Now let's use that model into AI world. There is the AI shared responsibility model, which is fairly new, but also there it states that organizations are always responsible for securing their data, which is then used to train models. No, it's not the responsibility of the cloud provider. So what role would a PET then play into the shared responsibility? Like would you say PETs are very important to cover the aspect of you are responsible for your data? That's a very interesting question. So the reason why, because again, it's critical, right? So as these organizations have this own, the shared responsibilities in place, I think the PETs are again, kind of critical because, it kind of supports both parties to ensure that the data is privacy, the privacy and the compliance aspect in place. So if we think of that, the shared responsibility model from a cloud service provider standpoint, so I think within that, in common we see there's infrastructure security, that again, within infrastructure security it includes the hardware, storage and the network aspect. And then there is the customer slash the consumer responsibility. So where our responsibility is to secure the data and applications that are hosted in the data and managing the identity and access controls and encryption keys and the data collection phase that I was talking about on the data lifecycle, starting with collection processing, storage and deletion, and again, ensuring that the compliance or the privacy regulations when using the cloud resources. So again, kind of thinking to the PET in the shared responsibility space, right? So again, going back to the data collection and uploading, I think it really centers in terms of the data consumer. So I think, I would say the majority of the ownership really relies with the consumer as a user than the company provider. Of course, company provider has its own metrics that they're already in place, but again, it's the organization's responsibility to bring in the PETs, the differential privacy, the homomorphic encryption, and in all the phases of that, the AI lifecycles. So yeah, I think it's always good to have these shared responsibility document outlined, or again, kind of touching to the data governance aspect, defining the roles are very critical as part of the data governance. And then especially when you're using the third party service providers, how do you make sure that you're bringing them to the conversation from an early phase, not just during the deployment phase? So one of the challenges that we had in my previous company is so we have managed to build the product from concept to launch, but at the end, we reached out to the cloud service provider, and then we were trying to figure out how best we can kind of integrate our existing solution within their existing system. But then they have their own shared data agreements. They have their own shared data policies, which again, which we really didn't take into account while we are building the project. So I think it's always good practice to at least have these conversations with the service provider or the cloud infrastructure service provider in an early phase. Again, it doesn't have to be very specific. Maybe you can start with the conversation just to give them some context. OK, this is what we are trying to achieve. This is what we are expecting by end of the project. So that they have some awareness, and they can also think or maybe assigning some computational resources or the team and staffing. It's always good to bring both parties in early phases of the development process. Great, great. I like the fact that you also indeed confirmed what I already thought that indeed for the shared AI responsibility, an organization remains responsible for securing their data, which will be used to train AI models. And a way to secure it is using PETs. So basically, they are controls. So to the audience, if you're a CISO, you're a DPO, you're responsible for securing data. PETs is the way to go from beginning till the end. In all the lifecycle, you can use them. Now, what I've seen so far on my research is there are many documents and examples describing individual PETs. This is a differential privacy, and this is a federated learning. But I still don't find the roadmap or, I say, the decision tree for a CISO to make a decision, like which one should I use first and when. So that is exactly the point that I was asking you for. And I like the book there. It's already a great primer. So talking about the book and slowly wrapping up the podcast, how is the book doing? Again, it's so interesting. We have been getting a lot of attention on the book, and several US universities are indexed the book into their existing libraries. So I think in terms of engagement, and I think it has almost reached over 3,000 plus accesses since the release of the book, I think it kind of sparked significant and reaching to the global audience where the strong reach within US, Europe, and Asia, where the privacy and regulations like GDPR and AI governance frameworks are the major topics of interest. And we have been getting pretty positive feedback from the readers, especially the way that we outlined the PETs. Again, not many books in this space. Really focusing on the PET aspect. And so we got one of the good feedback from the IT leaders where it kind of served as their practical guidance in terms of around the difference of privacy for learning the homomorphic encryption aspect. And one of the other interesting misses that I got from a startup based in Silicon Valley is they appreciated the fact of the different case studies that we presented within the book. And they found it quite helpful, which again shows promising results. And then on the research and academic community, again, there is a lot as AI is growing and now with the ethical AI coming into place. So how do we balance that ethical AI and privacy preserving methodologies that kind of aligns or make sure that it aligns with the current trends in the responsible AI space? I think institutions and researchers are also leveraging it so fascinating to see how different audience from academic, from corporate, and from startups and how they are leveraging. And on the PET, I'm actually putting together a short concept note for a public sector and for policymakers specifically to understand the role of AI in safeguarding public and citizens data while promoting that innovation aspect. And I think I feel like it's doing very good. And Ranadeep and I are so happy with the feedback. And again, that kind of gives us a push to write more around the PETs. Wonderful, wonderful. And I got to be honest with you, I'm doing a master's of science in IT risk and cybersecurity management. And my research paper is on PETs from a practical perspective, how to select them, what to look for. And I'm using your book as also a key reference there. And also, of course, this interview. So it's a double benefit there. So thank you so much for the book. Last question, if our listeners want to reach out to you for more information or perhaps some consulting, how can they best reach you? Sure. Yeah, I think one of the best ways-- of course, LinkedIn. So they can always reach out to me over LinkedIn. And of course, they can also reach out to me over my email, which is kkathala@umass.edu. That's something I've always checked. And of course, I'm happy to share my personal email ID if any folks are interested. So, yeah. Wonderful, wonderful. We'll also put the link to your book in the podcast itself in the final production. We'll also put your LinkedIn page there as well. So for those listening, 'Privacy in the Age of Innovation, AI Solutions for Information Security' by Krishna and Rana Deep and published by Apress. Really good book. Go pick it up. We'll put the link in the description as well. Krishna, thank you very much for joining this podcast and your wonderful insights. I wish you all the best and perhaps we'll see each other again. Thank you so much. And thank you so much for having me here and I look forward how organizations can best take from this discussion. And thank you, thank you so much. Wonderful.

---

## Book Info

**Title:** *Privacy in the Age of Innovation: AI Solutions for Information Security*  
**Authors:** Krishna Chaitanya Rao Kathala & Ranadeep Reddy Palle  
**Publisher:** Apress  
**Link:** https://www.amazon.com/Privacy-Age-Innovation-Solutions-Information-ebook/dp/B0D5KNV1VZ

---

